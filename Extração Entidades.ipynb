{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-11-19T22:37:22.510914Z",
     "iopub.status.busy": "2021-11-19T22:37:22.510517Z",
     "iopub.status.idle": "2021-11-19T22:37:22.515901Z",
     "shell.execute_reply": "2021-11-19T22:37:22.515222Z",
     "shell.execute_reply.started": "2021-11-19T22:37:22.510890Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline, BertForTokenClassification, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from gensim.utils import simple_preprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sent):\n",
    "    sent = re.sub(\"\\s+\", \" \", sent)  # remove newline chars\n",
    "    sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "    sent = re.sub(\"  \", \" \", sent)  # remove single quotes\n",
    "    #sent = simple_preprocess(str(sent), deacc=True) \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T23:22:58.842570Z",
     "iopub.status.busy": "2021-11-19T23:22:58.842208Z",
     "iopub.status.idle": "2021-11-19T23:22:58.846964Z",
     "shell.execute_reply": "2021-11-19T23:22:58.846002Z",
     "shell.execute_reply.started": "2021-11-19T23:22:58.842546Z"
    }
   },
   "outputs": [],
   "source": [
    "MODELS_DIR = [\n",
    "    r\"pucpr/clinicalnerpt-diagnostic\",\n",
    "    r\"pucpr/clinicalnerpt-disease\",           \n",
    "    r\"pucpr/clinicalnerpt-sign\",\n",
    "    r\"pucpr/clinicalnerpt-disorder\",             \n",
    "    r\"pucpr/clinicalnerpt-finding\"   \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-20T00:02:48.956826Z",
     "iopub.status.busy": "2021-11-20T00:02:48.956591Z",
     "iopub.status.idle": "2021-11-20T00:02:48.971929Z",
     "shell.execute_reply": "2021-11-20T00:02:48.971451Z",
     "shell.execute_reply.started": "2021-11-20T00:02:48.956803Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertForTokenClassification, BertConfig\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import nltk    \n",
    "from nltk import tokenize  \n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def predictBERTNER(sentencas, MODEL_DIR):\n",
    "        \n",
    "    model = BertForTokenClassification.from_pretrained(MODEL_DIR)\n",
    "    #tokenizer = BertTokenizer.from_pretrained(MODEL_DIR, do_lower_case=True) # lower or not, this is important\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"pucpr/biobertpt-all\", do_lower_case=True, truncation=True) # lower or not, this is important\n",
    "    #config = BertConfig.from_pretrained(MODEL_DIR, num_labels=3)\n",
    "    config = BertConfig.from_pretrained(MODEL_DIR)\n",
    "    #print(config.id2label)\n",
    "    #print(type(config.id2label))\n",
    "    dic_label=config.id2label\n",
    "    \n",
    "    predictedModel=[]\n",
    "    \n",
    "    for test_sentence in sentencas:\n",
    "        tokenized_sentence = tokenizer.encode(test_sentence)\n",
    "        input_ids = torch.tensor([tokenized_sentence])#.cuda()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids)\n",
    "        label_indices = np.argmax(output[0].to('cpu').numpy(), axis=2)\n",
    "        \n",
    "        # join bpe split tokens\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\n",
    "        new_tokens, new_labels = [], []\n",
    "        for token, label_idx in zip(tokens, label_indices[0]):\n",
    "            if token.startswith(\"##\"):\n",
    "                new_tokens[-1] = new_tokens[-1] + token[2:]\n",
    "            else:\n",
    "                new_labels.append(label_idx)\n",
    "                new_tokens.append(token)\n",
    "            \n",
    "        FinalLabelSentence = []\n",
    "        for token, label in zip(new_tokens, new_labels):\n",
    "            label = dic_label.get(label)\n",
    "            if label == \"O\" or label == \"X\":\n",
    "                FinalLabelSentence.append(\"O\")\n",
    "            else:\n",
    "                FinalLabelSentence.append(label)\n",
    "                \n",
    "        predictedModel.append(FinalLabelSentence[1:-1]) # delete [SEP] and [CLS]\n",
    "        \n",
    "            \n",
    "    return predictedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T23:17:09.217846Z",
     "iopub.status.busy": "2021-11-19T23:17:09.217159Z",
     "iopub.status.idle": "2021-11-19T23:17:09.314670Z",
     "shell.execute_reply": "2021-11-19T23:17:09.313590Z",
     "shell.execute_reply.started": "2021-11-19T23:17:09.217812Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('sample_laudos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o estudo tomográfico computadorizado do tórax realizado com cortes axiais com a injeção ev de contraste mostra: extenso pneumotórax à direita com colapso do lobo pulmonar ipsilateral e áreas de enfisema subcutâneo arcos costais adjacentes de difícil avaliação por artefato de movimento opacidades pulmonares posteriores (de decúbito) restante do parênquima pulmonar com coeficientes de atenuação conservados. vias aéreas de calibre e aspecto preservados. mediastino centrado sem evidências de massas ou adenomegalias. coração de contornos e dimensões normais. aorta torácica de calibre mantido. estruturas hilares normoconfiguradas. tronco da artéria pulmonar e ramos principais direito e esquerdo pérvios de calibre mantido.'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-19T23:58:04.092035Z",
     "iopub.status.busy": "2021-11-19T23:58:04.091823Z",
     "iopub.status.idle": "2021-11-19T23:58:04.096545Z",
     "shell.execute_reply": "2021-11-19T23:58:04.095579Z",
     "shell.execute_reply.started": "2021-11-19T23:58:04.092013Z"
    }
   },
   "outputs": [],
   "source": [
    "# THE MODEL ACCEPTS ONLY LOWER\n",
    "test_sentence = sent_to_words(dataset['laudo_completo'][70])\n",
    "test_sentence1 = test_sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictedModels = []\n",
    "for model_name in MODELS_DIR:\n",
    "    try:   \n",
    "\n",
    "        #tag_values = tags_values[i]\n",
    "\n",
    "        #print(tag_values)\n",
    "        ner_results = predictBERTNER([test_sentence], model_name)\n",
    "        predictedModels.append(ner_results[0])\n",
    "\n",
    "    except:\n",
    "        print('Erro BERT (frase muito grande provavelmente)')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['O', 'B-DiagnosticProcedure', 'I-DiagnosticProcedure', 'I-DiagnosticProcedure', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-DiseaseOrSyndrome', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'I-Finding', 'I-Finding', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']]\n"
     ]
    }
   ],
   "source": [
    "print(predictedModels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictedModels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
